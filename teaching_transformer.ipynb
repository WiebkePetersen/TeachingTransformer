{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a82070a",
   "metadata": {},
   "source": [
    "# Attention mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e38e7be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of basic functions\n",
    "\n",
    "# vector addition\n",
    "def vectorAdd(v1, v2):\n",
    "    if len(v1) != len(v2):\n",
    "        raise ValueError(\"different vector lengths\")\n",
    "    return [v1[i] + v2[i] for i in range(len(v1))]\n",
    "\n",
    "\n",
    "# vector addition for a list of vectors\n",
    "def vectorAddList(vectorlist):\n",
    "    v= vectorlist[0]\n",
    "    for vi in vectorlist[1:]:\n",
    "        v = vectorAdd(v, vi)\n",
    "    return v\n",
    "\n",
    "# scalar multiplication\n",
    "def scalarMult(scalar, v):\n",
    "    return [scalar * v[i] for i in range(len(v))]\n",
    "\n",
    "# dot product\n",
    "def dot(v1, v2):\n",
    "    if len(v1) != len(v2):\n",
    "        raise ValueError(\"different vector lengths\")\n",
    "    return sum(v1[i] * v2[i] for i in range(len(v1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2c1aaf48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a + b = [2, 2, 4] [2, 2, 4]\n",
      "2*a = [2, 4, 6]\n",
      "a . b = 4\n"
     ]
    }
   ],
   "source": [
    "a= [1,2,3] \n",
    "b= [1,0,1]\n",
    "print(\"a + b =\", vectorAdd(a, b), vectorAddList([a,b]))\n",
    "print(\"2*a =\", scalarMult(2, a))\n",
    "print(\"a . b =\", dot(a, b))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055f3cf3",
   "metadata": {},
   "source": [
    "Softmax Funktion $\\sigma$:\n",
    "\n",
    "$\\sigma: \\mathbf{R}^K \\rightarrow (0,1)^K$ normalisierte Exponentialfunktion mit \n",
    "\n",
    "$ \\sigma(\\vec{x})_i = \\frac{e^{x_{i}}}{\\sum_{j=1}^K e^{x_{j}}}  \\ for\\ i=1,2,\\dots,K  $\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6f9a7f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "e = 2.718281828459\n",
    "\n",
    "def softmax(x):\n",
    "    return [e**x[i] / sum(e**x[j] for j in range(len(x))) for i in range(len(x))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6fd8c0b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3] softmax = [0.09003057317038284, 0.24472847105480003, 0.6652409557748171]\n",
      "[1, 0, 1] softmax = [0.4223187982515171, 0.1553624034969658, 0.4223187982515171]\n",
      "[1, -10, -10] softmax = [0.576116884765825, 0.21194155761708747, 0.21194155761708747]\n",
      "[4] softmax = [1.0]\n"
     ]
    }
   ],
   "source": [
    "print(a,\"softmax =\", softmax(a))\n",
    "print(b,\"softmax =\", softmax(b))\n",
    "print([1,-10,-10],\"softmax =\", softmax([1,0,0]))\n",
    "print([4],\"softmax =\", softmax([4]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44873534",
   "metadata": {},
   "source": [
    "$\\alpha$ ist die Relevanzmatrix, die sich aus dem Vergleich der Wortvektoren untereinander ergibt. Sei $L$ die Eingangstextlänge, dann ist\n",
    "\n",
    "$\\alpha_{ij} = softmax(score(x_i,x_j))\\ \\forall i,j \\leq K$ die Relevanzmatrix.\n",
    "\n",
    "Im Falle von *masked attention* vergleicht man den Wortvektor nur mit den vorangegangenen. Damit ergeben sich für ein Wort $x_i$ die Relevanzwerte:\n",
    "\n",
    "$\\alpha_{ij} = softmax(score(x_i,x_j))\\ \\forall j \\leq i$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a31345a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(x,y):\n",
    "    return dot(x,y)\n",
    "\n",
    "def alpha_masked(x): # x ist eine Liste von Wortvektoren\n",
    "    alpha = []\n",
    "    for i in range(len(x)):\n",
    "        row = []\n",
    "        for j in range(i+1):\n",
    "            row.append(score(x[i], x[j]))\n",
    "        alpha.append(softmax(row))\n",
    "    return alpha\n",
    "    \n",
    "def alpha(x): # x ist eine Liste von Wortvektoren\n",
    "    alpha = []\n",
    "    for i in range(len(x)):\n",
    "        row = []\n",
    "        for j in range(len(x)):\n",
    "            row.append(score(x[i], x[j]))\n",
    "        alpha.append(softmax(row))\n",
    "    return alpha\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e1fd8a53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[0.9999484585145457, 4.539758978267296e-05, 6.143895671497805e-06],\n",
       "  [0.9908674725821718, 0.006676412513377005, 0.002456114904451198],\n",
       "  [0.9646631559719018, 0.017668422014049192, 0.017668422014049192]],\n",
       " [[1.0],\n",
       "  [0.9933071490757145, 0.006692850924285412],\n",
       "  [0.9646631559719018, 0.017668422014049192, 0.017668422014049192]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorlist = [[2,4], [1,2], [0,2]]\n",
    "alpha(vectorlist), alpha_masked(vectorlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb9286f",
   "metadata": {},
   "source": [
    "Der kontextualisierte Output-Vektor  $y_i$ zu einem Inputvektor $x_i$ gegeben eine Liste von Wortvektoren $[x_j\\ j\\leq i]$ ist:\n",
    "\n",
    "$$ y_i = \\sum_{j\\leq i} \\alpha_{i,j} x_j  $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "70d1ecc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def contextualized(x): # x ist eine Liste von Wortvektoren\n",
    "    y = []\n",
    "    for i in range(len(x)):\n",
    "        a = alpha_masked(x)\n",
    "        yi = vectorAddList([scalarMult(a[i][j] ,x[j]) for j in range(i+1)])\n",
    "        y.append(yi)\n",
    "    return y\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "cfde19d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[2, 4], [1, 2], [2, 0.1]],\n",
       " [[2.0, 4.0],\n",
       "  [1.9933071490757144, 3.9866142981514288],\n",
       "  [1.938024701975659, 2.399131881320575]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorlist = [[2, 4], [1, 2], [2, 0.1]]\n",
    "vectorlist, contextualized(vectorlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e86602",
   "metadata": {},
   "source": [
    "Liste von Wortvektoren: $[[2, 4], [1, 2], [2, 0.1]]$\n",
    "Detaillierte Berechnung von $$ y_2 = \\sum_{j\\leq 2} \\alpha_{2,j} x_j = \\alpha_{2,0} x_0 + \\alpha_{2,1} x_1 + \\alpha_{2,2} x_2 $$ \n",
    "$$ [\\alpha_{2,0},\\alpha_{2,1},\\alpha_{2,2}] = softmax([score(x_2,x_0), score(x_2,x_1), score(x_2,x_2)] )  = \\\\\n",
    "softmax([4.4,2.2,4.01]) \\approx [0.56,0.06,0.38] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3adf535a",
   "metadata": {},
   "source": [
    "**Vorsicht**: Der Vektor [2,0.1] hat ein höheres dot-Produkt mit dem Vektor [2,4] als mit sich selbst. Dies widerspricht der Idee, dass das dot-Produkt hier Ähnlichkeit modellieren soll. Ein Ausweg ist die explizite L2-Normalisierung der Wort-Embeddings:\n",
    "Bevor das Dot-Produkt zur Berechnung von Relevanz-Scores verwendet wird, werden die Wortvektoren (Embeddings) explizit auf die Länge 1 normiert.\n",
    "\n",
    "Statt einem Vektor $u$ wird der Vektor $\\frac{u}{\\|u\\|}$ genutzt. Damit wird das dot-Produkt zur Cosinus-Ähnlichkeit: \n",
    "$$\\text{cosine\\_similarity}(u, v) = \\frac{u \\cdot v}{\\|u\\| \\cdot \\|v\\|}$$\n",
    "\n",
    "Hier werden wir dieses Problem erst einmal vernachlässigen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c22a6894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Focus: embedding of 'chair'\n",
      "uncontextualized embedding of chair is [3, 4, 3]\n",
      "uncontextualized embedding of session is [7, 2, 1]\n",
      "uncontextualized embedding of person is [1, 2, 7]\n",
      "\n",
      "contextualized embedding of chair in ['each', 'session', 'has', 'a', 'chair'] is\n",
      "[3.488241706560337, 3.3861879899594367, 3.1255703034802282]\n",
      "\n",
      "contextualized embedding of chair in ['each', 'person', 'has', 'a', 'chair'] is\n",
      "[3.1255703034802282, 3.3861879899594367, 3.4882417065603373]\n"
     ]
    }
   ],
   "source": [
    "w1 = \"each\"\n",
    "wa2 = \"session\"\n",
    "wb2 = \"person\"\n",
    "w3 = \"has\"\n",
    "w4 = \"a\"\n",
    "w5 = \"chair\"\n",
    "\n",
    "\n",
    "\n",
    "v1 = [4,3,3]\n",
    "va2 = [7,2,1]\n",
    "vb2 = [1,2,7]\n",
    "v3 = [3.5,3,3.5]\n",
    "v4 = [3,3,4]\n",
    "v5 = [3,4,3]\n",
    "\n",
    "s1 = [w1, wa2, w3, w4, w5]\n",
    "s2 = [w1, wb2, w3, w4, w5]\n",
    "vs1 = [v1, va2, v3, v4, v5]\n",
    "vs2 = [v1, vb2, v3, v4, v5]\n",
    "\n",
    "print(\"Focus: embedding of '{}'\".format(w5))\n",
    "\n",
    "print(\"uncontextualized embedding of\", w5, \"is\", v5) \n",
    "print(\"uncontextualized embedding of\", wa2, \"is\", va2) \n",
    "print(\"uncontextualized embedding of\", wb2, \"is\", vb2) \n",
    "print(\"\\ncontextualized embedding of\", w5, \"in\", s1, \"is\")\n",
    "print(contextualized(vs1)[4])\n",
    "print(\"\\ncontextualized embedding of\", w5, \"in\", s2, \"is\")\n",
    "print(contextualized(vs2)[4])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2f6685",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
